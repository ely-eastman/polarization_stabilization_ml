{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac3c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access: ../Data/data_angles/txp_1551.5_pax_1552.5_polcon_and_fiber.csv\n",
      "Full path: C:\\Users\\Ely Eastman\\Documents\\polarization_stabilization_ml\\Data\\data_angles\\txp_1551.5_pax_1552.5_polcon_and_fiber.csv\n",
      "File found!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = \"../Data/data_angles/txp_1551.5_pax_1552.5_polcon_and_fiber.csv\"\n",
    "print(\"Attempting to access:\", data_path)\n",
    "print(\"Full path:\", os.path.abspath(data_path))\n",
    "if os.path.exists(data_path):\n",
    "    print(\"File found!\")\n",
    "else:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa19e251",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c735fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a43c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c046df10-911f-442c-a855-60acc50910a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "093610c3-4131-4d12-b6f7-7487dfb15af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA RTX A2000\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected, exiting!\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef81b8d-1f82-41e0-bed7-0704cda95b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access: ..\\Data\\data_angles\\txp_1551.5_pax_1552.5_polcon_and_fiber.csv\n",
      "Full path: C:\\Users\\Ely Eastman\\Documents\\polarization_stabilization_ml\\Data\\data_angles\\txp_1551.5_pax_1552.5_polcon_and_fiber.csv\n",
      "File found!\n",
      "\n",
      "Experimenting with window size: 8\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.1946, Test Loss: 0.6648\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1181, Test Loss: 0.5325\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.1088, Test Loss: 0.6961\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0961, Test Loss: 0.7239\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0838, Test Loss: 0.6352\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0727, Test Loss: 0.5686\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0661, Test Loss: 0.5977\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0646, Test Loss: 0.6975\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0623, Test Loss: 0.7035\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0625, Test Loss: 0.7201\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0616, Test Loss: 0.6807\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0586, Test Loss: 0.7235\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0591, Test Loss: 0.6335\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0579, Test Loss: 0.6525\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0576, Test Loss: 0.6640\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0564, Test Loss: 0.6089\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0554, Test Loss: 0.6287\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0545, Test Loss: 0.6245\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0549, Test Loss: 0.6613\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0539, Test Loss: 0.6283\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0539, Test Loss: 0.6295\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0534, Test Loss: 0.6960\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0516, Test Loss: 0.6401\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0525, Test Loss: 0.6967\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0506, Test Loss: 0.6171\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0511, Test Loss: 0.6163\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0506, Test Loss: 0.6375\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0498, Test Loss: 0.7018\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0493, Test Loss: 0.6646\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0486, Test Loss: 0.6656\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0493, Test Loss: 0.6766\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0475, Test Loss: 0.5935\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0474, Test Loss: 0.6233\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0464, Test Loss: 0.6332\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0462, Test Loss: 0.6519\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0462, Test Loss: 0.6214\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0456, Test Loss: 0.6510\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0454, Test Loss: 0.6353\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0444, Test Loss: 0.6580\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0444, Test Loss: 0.6322\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0433, Test Loss: 0.6217\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0437, Test Loss: 0.6202\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0434, Test Loss: 0.6364\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0436, Test Loss: 0.6376\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0435, Test Loss: 0.6335\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0435, Test Loss: 0.6452\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0431, Test Loss: 0.6392\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0427, Test Loss: 0.6371\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0424, Test Loss: 0.6359\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0424, Test Loss: 0.6373\n",
      "\n",
      "Experimenting with window size: 16\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.2018, Test Loss: 0.7013\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1190, Test Loss: 0.6083\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.1040, Test Loss: 0.6814\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0966, Test Loss: 0.7889\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0881, Test Loss: 0.7339\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0773, Test Loss: 0.5341\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0713, Test Loss: 0.5585\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0684, Test Loss: 0.6955\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0662, Test Loss: 0.6246\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0650, Test Loss: 0.5851\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0632, Test Loss: 0.6174\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0617, Test Loss: 0.6339\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0611, Test Loss: 0.7052\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0601, Test Loss: 0.6532\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0594, Test Loss: 0.5794\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0582, Test Loss: 0.6662\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0591, Test Loss: 0.6293\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0574, Test Loss: 0.6490\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0560, Test Loss: 0.6632\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0562, Test Loss: 0.6752\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0549, Test Loss: 0.7176\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0541, Test Loss: 0.5969\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0541, Test Loss: 0.6358\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0527, Test Loss: 0.6380\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0526, Test Loss: 0.6582\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0518, Test Loss: 0.6442\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0514, Test Loss: 0.6296\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0503, Test Loss: 0.6491\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0495, Test Loss: 0.6678\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0487, Test Loss: 0.6666\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0485, Test Loss: 0.6644\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0474, Test Loss: 0.6877\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0468, Test Loss: 0.6485\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0465, Test Loss: 0.6575\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0460, Test Loss: 0.6564\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0456, Test Loss: 0.6478\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0453, Test Loss: 0.6309\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0441, Test Loss: 0.6600\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0436, Test Loss: 0.6428\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0436, Test Loss: 0.6573\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0431, Test Loss: 0.6556\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0431, Test Loss: 0.6523\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0433, Test Loss: 0.6667\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0427, Test Loss: 0.6641\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0427, Test Loss: 0.6477\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0428, Test Loss: 0.6559\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0425, Test Loss: 0.6634\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0422, Test Loss: 0.6555\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0416, Test Loss: 0.6557\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0422, Test Loss: 0.6580\n",
      "\n",
      "Experimenting with window size: 32\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.1757, Test Loss: 0.5917\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1135, Test Loss: 0.6459\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.1016, Test Loss: 0.6582\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0953, Test Loss: 0.7319\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0833, Test Loss: 0.5941\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0725, Test Loss: 0.5689\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0705, Test Loss: 0.6034\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0676, Test Loss: 0.5879\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0643, Test Loss: 0.5983\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0634, Test Loss: 0.5686\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0623, Test Loss: 0.6114\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0626, Test Loss: 0.6469\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0610, Test Loss: 0.6473\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0603, Test Loss: 0.5903\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0595, Test Loss: 0.5828\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0596, Test Loss: 0.6254\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0580, Test Loss: 0.6613\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0575, Test Loss: 0.6540\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0569, Test Loss: 0.6374\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0570, Test Loss: 0.5806\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0561, Test Loss: 0.6979\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0552, Test Loss: 0.6889\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0551, Test Loss: 0.6442\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0539, Test Loss: 0.6147\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0531, Test Loss: 0.6764\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0524, Test Loss: 0.6521\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0515, Test Loss: 0.6439\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0501, Test Loss: 0.6745\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0490, Test Loss: 0.6451\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0482, Test Loss: 0.6625\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0463, Test Loss: 0.6598\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0462, Test Loss: 0.6306\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0454, Test Loss: 0.6895\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0447, Test Loss: 0.6664\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0433, Test Loss: 0.6407\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0425, Test Loss: 0.6748\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0420, Test Loss: 0.6604\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0414, Test Loss: 0.6667\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0411, Test Loss: 0.6788\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0409, Test Loss: 0.6628\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0398, Test Loss: 0.6572\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0395, Test Loss: 0.6860\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0397, Test Loss: 0.6452\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0387, Test Loss: 0.6601\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0394, Test Loss: 0.6572\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0390, Test Loss: 0.6580\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0388, Test Loss: 0.6595\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0389, Test Loss: 0.6630\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0388, Test Loss: 0.6649\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0386, Test Loss: 0.6624\n",
      "\n",
      "Experimenting with window size: 64\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.1604, Test Loss: 0.5682\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1098, Test Loss: 0.8681\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.0938, Test Loss: 0.8160\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0836, Test Loss: 0.5044\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0792, Test Loss: 0.6334\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0716, Test Loss: 0.5947\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0662, Test Loss: 0.6539\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0635, Test Loss: 0.6071\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0616, Test Loss: 0.6097\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0601, Test Loss: 0.5780\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0597, Test Loss: 0.6050\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0576, Test Loss: 0.6988\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0571, Test Loss: 0.5578\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0578, Test Loss: 0.6797\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0565, Test Loss: 0.6344\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0547, Test Loss: 0.5813\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0542, Test Loss: 0.6182\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0521, Test Loss: 0.6075\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0509, Test Loss: 0.6385\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0499, Test Loss: 0.5996\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0492, Test Loss: 0.5972\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0478, Test Loss: 0.6427\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0483, Test Loss: 0.6481\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0464, Test Loss: 0.6201\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0457, Test Loss: 0.6108\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0444, Test Loss: 0.6507\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0442, Test Loss: 0.6175\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0435, Test Loss: 0.6237\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0431, Test Loss: 0.6203\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0425, Test Loss: 0.6235\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0421, Test Loss: 0.6285\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0412, Test Loss: 0.6530\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0406, Test Loss: 0.6337\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0400, Test Loss: 0.6179\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0393, Test Loss: 0.6199\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0394, Test Loss: 0.6420\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0384, Test Loss: 0.6348\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0381, Test Loss: 0.6551\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0380, Test Loss: 0.6375\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0376, Test Loss: 0.6357\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0372, Test Loss: 0.6421\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0371, Test Loss: 0.6560\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0364, Test Loss: 0.6535\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0365, Test Loss: 0.6251\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0363, Test Loss: 0.6323\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0360, Test Loss: 0.6437\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0359, Test Loss: 0.6372\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0358, Test Loss: 0.6329\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0361, Test Loss: 0.6407\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0355, Test Loss: 0.6379\n",
      "\n",
      "Experimenting with window size: 128\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.2276, Test Loss: 0.7733\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1119, Test Loss: 0.7063\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.0987, Test Loss: 0.6040\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0848, Test Loss: 0.5533\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0736, Test Loss: 0.4860\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0637, Test Loss: 0.5933\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0598, Test Loss: 0.6177\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0559, Test Loss: 0.5250\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0551, Test Loss: 0.6171\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0531, Test Loss: 0.5944\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0517, Test Loss: 0.5287\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0516, Test Loss: 0.5536\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0494, Test Loss: 0.5753\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0486, Test Loss: 0.5618\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0473, Test Loss: 0.6104\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0451, Test Loss: 0.6163\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0421, Test Loss: 0.6275\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0417, Test Loss: 0.5863\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0389, Test Loss: 0.5944\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0363, Test Loss: 0.5810\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0331, Test Loss: 0.6888\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0322, Test Loss: 0.6532\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0310, Test Loss: 0.6151\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0290, Test Loss: 0.5997\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0276, Test Loss: 0.6256\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0270, Test Loss: 0.5992\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0261, Test Loss: 0.6238\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0250, Test Loss: 0.5696\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0253, Test Loss: 0.6382\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0240, Test Loss: 0.6227\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0230, Test Loss: 0.6175\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0227, Test Loss: 0.5943\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0221, Test Loss: 0.6315\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0212, Test Loss: 0.6411\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0213, Test Loss: 0.6331\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0205, Test Loss: 0.6313\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0196, Test Loss: 0.6400\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0196, Test Loss: 0.5912\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0193, Test Loss: 0.6194\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0180, Test Loss: 0.6389\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0181, Test Loss: 0.6313\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0175, Test Loss: 0.6339\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0178, Test Loss: 0.6407\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0175, Test Loss: 0.6327\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0172, Test Loss: 0.6350\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0176, Test Loss: 0.6326\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0174, Test Loss: 0.6281\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0169, Test Loss: 0.6392\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0165, Test Loss: 0.6338\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0171, Test Loss: 0.6328\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "Normalized Feature Phi range: -0.9987186243781412 to 0.9965726729942881\n",
      "Normalized Feature Theta range: 0.015206224514549647 to 0.9379400142695341\n",
      "Normalized Target Phi range: -0.9999310370204916 to 0.9999685975870611\n",
      "Normalized Target Theta range: 0.0546223351390938 to 0.9576266749150993\n",
      "\n",
      "Experimenting with model size: small (d_model=16, n_layers=1, n_heads=4)\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.3807, Test Loss: 0.4726\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.2207, Test Loss: 0.4671\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.2183, Test Loss: 0.4959\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.2164, Test Loss: 0.4710\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.2140, Test Loss: 0.4973\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.2098, Test Loss: 0.4836\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.2023, Test Loss: 0.5156\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.1795, Test Loss: 0.5589\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.1382, Test Loss: 0.5812\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.1297, Test Loss: 0.5469\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.1275, Test Loss: 0.5935\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.1250, Test Loss: 0.5960\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.1245, Test Loss: 0.5938\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.1235, Test Loss: 0.5864\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.1230, Test Loss: 0.5887\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.1225, Test Loss: 0.5810\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.1222, Test Loss: 0.5729\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.1217, Test Loss: 0.6135\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.1214, Test Loss: 0.5812\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.1213, Test Loss: 0.5849\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.1210, Test Loss: 0.5694\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.1206, Test Loss: 0.5831\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.1203, Test Loss: 0.5677\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.1205, Test Loss: 0.6021\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.1201, Test Loss: 0.5730\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.1197, Test Loss: 0.5775\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.1194, Test Loss: 0.5579\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.1196, Test Loss: 0.5611\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.1192, Test Loss: 0.5776\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.1194, Test Loss: 0.5690\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.1192, Test Loss: 0.5698\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.1191, Test Loss: 0.5796\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.1188, Test Loss: 0.5586\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.1190, Test Loss: 0.5516\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.1187, Test Loss: 0.5639\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.1185, Test Loss: 0.5781\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.1189, Test Loss: 0.5770\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.1185, Test Loss: 0.5762\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.1184, Test Loss: 0.5703\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.1185, Test Loss: 0.5656\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.1186, Test Loss: 0.5706\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.1186, Test Loss: 0.5722\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.1185, Test Loss: 0.5690\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.1185, Test Loss: 0.5803\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.1186, Test Loss: 0.5668\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.1184, Test Loss: 0.5663\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.1182, Test Loss: 0.5663\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.1183, Test Loss: 0.5681\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.1186, Test Loss: 0.5680\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.1184, Test Loss: 0.5681\n",
      "\n",
      "Experimenting with model size: medium (d_model=32, n_layers=2, n_heads=4)\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.2502, Test Loss: 0.4943\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1653, Test Loss: 0.6289\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.1240, Test Loss: 0.6273\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.1161, Test Loss: 0.6405\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.1108, Test Loss: 0.6696\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.1070, Test Loss: 0.6606\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.1035, Test Loss: 0.6424\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.1015, Test Loss: 0.6467\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0993, Test Loss: 0.6566\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0975, Test Loss: 0.6585\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0960, Test Loss: 0.6312\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0940, Test Loss: 0.6107\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0926, Test Loss: 0.6070\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0906, Test Loss: 0.6431\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0887, Test Loss: 0.5862\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0860, Test Loss: 0.6343\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0835, Test Loss: 0.6113\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0810, Test Loss: 0.6457\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0785, Test Loss: 0.6654\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0767, Test Loss: 0.6259\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0756, Test Loss: 0.6407\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0735, Test Loss: 0.6659\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0720, Test Loss: 0.6169\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0713, Test Loss: 0.6705\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0704, Test Loss: 0.6903\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0694, Test Loss: 0.6856\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0693, Test Loss: 0.6815\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0687, Test Loss: 0.7062\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0680, Test Loss: 0.6807\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0678, Test Loss: 0.6772\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0673, Test Loss: 0.7047\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0667, Test Loss: 0.6965\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0666, Test Loss: 0.6717\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0666, Test Loss: 0.6836\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0663, Test Loss: 0.6660\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0661, Test Loss: 0.6911\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0661, Test Loss: 0.6698\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0666, Test Loss: 0.6815\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0660, Test Loss: 0.6846\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0653, Test Loss: 0.6668\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0658, Test Loss: 0.6898\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0648, Test Loss: 0.6719\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0654, Test Loss: 0.6730\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0656, Test Loss: 0.6844\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0652, Test Loss: 0.6812\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0652, Test Loss: 0.6777\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0651, Test Loss: 0.6762\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0652, Test Loss: 0.6825\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0650, Test Loss: 0.6813\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0654, Test Loss: 0.6807\n",
      "\n",
      "Experimenting with model size: large (d_model=64, n_layers=3, n_heads=8)\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.2219, Test Loss: 0.5961\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1230, Test Loss: 0.6951\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.1141, Test Loss: 0.6872\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.1069, Test Loss: 0.5998\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.1012, Test Loss: 0.6831\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0973, Test Loss: 0.6946\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0910, Test Loss: 0.6504\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0866, Test Loss: 0.6370\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0802, Test Loss: 0.6251\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0757, Test Loss: 0.6439\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0716, Test Loss: 0.6166\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0681, Test Loss: 0.6505\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0665, Test Loss: 0.6196\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0652, Test Loss: 0.6338\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0640, Test Loss: 0.6161\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0626, Test Loss: 0.6309\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0621, Test Loss: 0.6300\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0613, Test Loss: 0.6232\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0608, Test Loss: 0.6734\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0605, Test Loss: 0.6502\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0605, Test Loss: 0.6837\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0603, Test Loss: 0.6365\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0591, Test Loss: 0.6274\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0587, Test Loss: 0.6035\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0580, Test Loss: 0.6491\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0580, Test Loss: 0.6483\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0573, Test Loss: 0.6653\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0572, Test Loss: 0.6199\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0571, Test Loss: 0.6737\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0571, Test Loss: 0.6489\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0566, Test Loss: 0.6346\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0566, Test Loss: 0.6329\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0563, Test Loss: 0.6180\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0558, Test Loss: 0.6484\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0558, Test Loss: 0.6369\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0556, Test Loss: 0.6407\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0561, Test Loss: 0.6516\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0553, Test Loss: 0.6384\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0555, Test Loss: 0.6488\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0557, Test Loss: 0.6483\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0552, Test Loss: 0.6459\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0549, Test Loss: 0.6491\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0546, Test Loss: 0.6523\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0550, Test Loss: 0.6485\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0548, Test Loss: 0.6505\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0549, Test Loss: 0.6486\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0544, Test Loss: 0.6498\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0548, Test Loss: 0.6495\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0547, Test Loss: 0.6486\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0547, Test Loss: 0.6482\n",
      "\n",
      "Experimenting with model size: extra_large (d_model=140, n_layers=4, n_heads=10)\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.2409, Test Loss: 0.5435\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.1149, Test Loss: 0.5867\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.1003, Test Loss: 0.6187\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.0872, Test Loss: 0.6285\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.0744, Test Loss: 0.6613\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.0679, Test Loss: 0.6500\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.0648, Test Loss: 0.5837\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.0629, Test Loss: 0.6105\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.0614, Test Loss: 0.6745\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.0605, Test Loss: 0.6635\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.0599, Test Loss: 0.6270\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.0603, Test Loss: 0.6880\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.0581, Test Loss: 0.6558\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.0591, Test Loss: 0.6679\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.0574, Test Loss: 0.6852\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.0581, Test Loss: 0.6425\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.0555, Test Loss: 0.6719\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.0561, Test Loss: 0.6765\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.0539, Test Loss: 0.6617\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.0538, Test Loss: 0.6677\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.0535, Test Loss: 0.6834\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.0515, Test Loss: 0.6683\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.0505, Test Loss: 0.6634\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.0510, Test Loss: 0.6081\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.0492, Test Loss: 0.6466\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.0489, Test Loss: 0.6359\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.0476, Test Loss: 0.5705\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.0470, Test Loss: 0.6086\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.0462, Test Loss: 0.6284\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.0450, Test Loss: 0.6307\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.0444, Test Loss: 0.6223\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.0435, Test Loss: 0.6351\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.0436, Test Loss: 0.6496\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.0430, Test Loss: 0.6259\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.0425, Test Loss: 0.6021\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.0424, Test Loss: 0.6465\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.0410, Test Loss: 0.6517\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.0409, Test Loss: 0.6599\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.0412, Test Loss: 0.6391\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.0402, Test Loss: 0.6401\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.0401, Test Loss: 0.6139\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.0399, Test Loss: 0.6462\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.0398, Test Loss: 0.6162\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.0395, Test Loss: 0.6416\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0396, Test Loss: 0.6340\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.0395, Test Loss: 0.6422\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.0393, Test Loss: 0.6371\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.0391, Test Loss: 0.6372\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.0391, Test Loss: 0.6356\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.0390, Test Loss: 0.6365\n"
     ]
    }
   ],
   "source": [
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, data_path: str, window_size: int, indices: np.ndarray = None):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        if self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                   'Target Phi (degrees)', 'Target Theta (degrees)']].isna().any().any():\n",
    "            print(\"Warning: NaN values found in data!\")\n",
    "            self.df = self.df.dropna()\n",
    "        if np.isinf(self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                            'Target Phi (degrees)', 'Target Theta (degrees)']].values).any():\n",
    "            print(\"Warning: Infinite values found in data!\")\n",
    "            self.df = self.df[~np.isinf(self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                                                'Target Phi (degrees)', 'Target Theta (degrees)']]).any(axis=1)]\n",
    "        \n",
    "        self.features = self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)']].values\n",
    "        self.targets = self.df[['Target Phi (degrees)', 'Target Theta (degrees)']].values\n",
    "        self.features[:, 0] /= 180.0\n",
    "        self.features[:, 1] /= 180.0\n",
    "        self.targets[:, 0] /= 180.0\n",
    "        self.targets[:, 1] /= 180.0\n",
    "        \n",
    "        print(\"Normalized Feature Phi range:\", self.features[:, 0].min(), \"to\", self.features[:, 0].max())\n",
    "        print(\"Normalized Feature Theta range:\", self.features[:, 1].min(), \"to\", self.features[:, 1].max())\n",
    "        print(\"Normalized Target Phi range:\", self.targets[:, 0].min(), \"to\", self.targets[:, 0].max())\n",
    "        print(\"Normalized Target Theta range:\", self.targets[:, 1].min(), \"to\", self.targets[:, 1].max())\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.indices = indices if indices is not None else np.arange(len(self.df) - window_size + 1)\n",
    "        self.length = len(self.indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        data_idx = self.indices[idx]\n",
    "        window = self.features[data_idx:data_idx + self.window_size]\n",
    "        target = self.targets[data_idx + self.window_size - 1]\n",
    "        time_index = data_idx + self.window_size - 1\n",
    "        return torch.FloatTensor(window), torch.FloatTensor(target), time_index\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        scale = 1.0 / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class AnglePredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, d_model: int, n_heads: int, n_layers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 128, d_model) * 0.1)\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            FlashAttention(d_model, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm_layers = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(d_model, 2)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        for attn, norm in zip(self.attn_layers, self.norm_layers):\n",
    "            residual = x\n",
    "            x = attn(x)\n",
    "            x = norm(x + residual)\n",
    "        \n",
    "        x = x[:, -1, :]\n",
    "        return self.output(x)\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, \n",
    "                epochs: int, device: torch.device, lr: float = 1e-4):\n",
    "    print(\"Model device:\", next(model.parameters()).device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (batch_x, batch_y, _) in enumerate(train_loader):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            if i == 0:\n",
    "                print(\"Batch x device:\", batch_x.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                output = model(batch_x)\n",
    "                if torch.isnan(output).any():\n",
    "                    print(f\"NaN in output at epoch {epoch+1}, batch {i}\")\n",
    "                    break\n",
    "                loss = criterion(output, batch_y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, _ in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                test_loss += criterion(output, batch_y).item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "def plot_predictions(predictions: np.ndarray, actuals: np.ndarray, time_indices: np.ndarray, \n",
    "                     rmse_phi: float, rmse_theta: float, filename: str, title_prefix: str):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(time_indices, actuals[:, 0], label='Actual Phi (1551 nm, )', color='blue', linewidth=2)\n",
    "    plt.plot(time_indices, predictions[:, 0], label='Predicted Phi (from 1531 nm, )', color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Phi Time Series (RMSE: {rmse_phi:.2f})')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Phi (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(time_indices, actuals[:, 1], label='Actual Theta (1551 nm, )', color='blue', linewidth=2)\n",
    "    plt.plot(time_indices, predictions[:, 1], label='Predicted Theta (from 1531 nm, )', color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Theta Time Series (RMSE: {rmse_theta:.2f})')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Theta (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    errors = np.abs(predictions - actuals)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(time_indices, errors[:, 0], label='|Predicted - Actual| Phi', color='purple', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Phi Absolute Error')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Error (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(time_indices, errors[:, 1], label='|Predicted - Actual| Theta', color='purple', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Theta Absolute Error')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Error (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'errors_{filename.split(\"_\", 1)[1]}')\n",
    "    plt.close()\n",
    "\n",
    "def experiment_window_sizes(data_path: str, window_sizes: list, batch_size: int = 64, epochs: int = 50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = {}\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        print(f\"\\nExperimenting with window size: {window_size}\")\n",
    "        \n",
    "        dataset_full = AngleDataset(data_path, window_size)\n",
    "        total_length = len(dataset_full)\n",
    "        train_length = int(0.8 * total_length)\n",
    "        test_length = total_length - train_length\n",
    "        \n",
    "        train_indices = np.arange(train_length)\n",
    "        test_indices = np.arange(train_length, total_length)\n",
    "        \n",
    "        train_dataset = AngleDataset(data_path, window_size, train_indices)\n",
    "        test_dataset = AngleDataset(data_path, window_size, test_indices)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = AnglePredictionModel(\n",
    "            input_dim=2,\n",
    "            d_model=140,\n",
    "            n_heads=4,\n",
    "            n_layers=4,\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "        \n",
    "        train_losses, test_losses = train_model(\n",
    "            model, train_loader, test_loader, epochs, device\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        time_indices = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_indices in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                output = output.cpu().numpy() * 180.0\n",
    "                batch_y = batch_y.cpu().numpy() * 180.0\n",
    "                predictions.append(output)\n",
    "                actuals.append(batch_y)\n",
    "                time_indices.append(batch_indices.numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        actuals = np.concatenate(actuals, axis=0)\n",
    "        time_indices = np.concatenate(time_indices, axis=0)\n",
    "        \n",
    "        sort_idx = np.argsort(time_indices)\n",
    "        time_indices = time_indices[sort_idx]\n",
    "        predictions = predictions[sort_idx]\n",
    "        actuals = actuals[sort_idx]\n",
    "        \n",
    "        rmse_phi = np.sqrt(np.mean((predictions[:, 0] - actuals[:, 0])**2))\n",
    "        rmse_theta = np.sqrt(np.mean((predictions[:, 1] - actuals[:, 1])**2))\n",
    "        \n",
    "        plot_predictions(\n",
    "            predictions, actuals, time_indices, rmse_phi, rmse_theta,\n",
    "            f'predictions_window_{window_size}.png',\n",
    "            f'Window Size {window_size}'\n",
    "        )\n",
    "        \n",
    "        results[window_size] = {\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals,\n",
    "            'time_indices': time_indices,\n",
    "            'rmse_phi': rmse_phi,\n",
    "            'rmse_theta': rmse_theta\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.title(f'Loss Curves (Window Size: {window_size})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_window_{window_size}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for window_size, result in results.items():\n",
    "        plt.plot(result['test_losses'], label=f'Window {window_size}')\n",
    "    plt.title('Test Loss Comparison Across Window Sizes')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('window_size_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def experiment_model_sizes(data_path: str, window_size: int = 64, batch_size: int = 64, epochs: int = 50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_configs = [\n",
    "        {'name': 'small', 'd_model': 16, 'n_heads': 4, 'n_layers': 1},\n",
    "        {'name': 'medium', 'd_model': 32, 'n_heads': 4, 'n_layers': 2},\n",
    "        {'name': 'large', 'd_model': 64, 'n_heads': 8, 'n_layers': 3},\n",
    "        # Extra Large: Increase d_model to ~140 to get ~5x parameters\n",
    "        {'name': 'extra_large', 'd_model': 140, 'n_heads': 10, 'n_layers': 4}\n",
    "    ]\n",
    "    results = {}\n",
    "    \n",
    "    dataset_full = AngleDataset(data_path, window_size)\n",
    "    total_length = len(dataset_full)\n",
    "    train_length = int(0.8 * total_length)\n",
    "    test_length = total_length - train_length\n",
    "\n",
    "    train_indices = np.arange(train_length)\n",
    "    test_indices = np.arange(train_length, total_length)\n",
    "\n",
    "    train_dataset = AngleDataset(data_path, window_size, train_indices)\n",
    "    test_dataset = AngleDataset(data_path, window_size, test_indices)\n",
    "\n",
    "    rmse_phi_list = []\n",
    "    rmse_theta_list = []\n",
    "    model_names = []\n",
    "\n",
    "    for config in model_configs:\n",
    "        model_name = config['name']\n",
    "        print(f\"\\nExperimenting with model size: {model_name} (d_model={config['d_model']}, \"\n",
    "              f\"n_layers={config['n_layers']}, n_heads={config['n_heads']})\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = AnglePredictionModel(\n",
    "            input_dim=2,\n",
    "            d_model=config['d_model'],\n",
    "            n_heads=config['n_heads'],\n",
    "            n_layers=config['n_layers'],\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "        \n",
    "        train_losses, test_losses = train_model(\n",
    "            model, train_loader, test_loader, epochs, device\n",
    "        )\n",
    "        \n",
    "        # Evaluation Phase\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        time_indices = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_indices in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                output = output.cpu().numpy() * 180.0\n",
    "                batch_y = batch_y.cpu().numpy() * 180.0\n",
    "                predictions.append(output)\n",
    "                actuals.append(batch_y)\n",
    "                time_indices.append(batch_indices.numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        actuals = np.concatenate(actuals, axis=0)\n",
    "        time_indices = np.concatenate(time_indices, axis=0)\n",
    "        \n",
    "        sort_idx = np.argsort(time_indices)\n",
    "        predictions = predictions[sort_idx]\n",
    "        actuals = actuals[sort_idx]\n",
    "        \n",
    "        rmse_phi = np.sqrt(np.mean((predictions[:, 0] - actuals[:, 0])**2))\n",
    "        rmse_theta = np.sqrt(np.mean((predictions[:, 1] - actuals[:, 1])**2))\n",
    "        \n",
    "        rmse_phi_list.append(rmse_phi)\n",
    "        rmse_theta_list.append(rmse_theta)\n",
    "        model_names.append(model_name.capitalize())\n",
    "        \n",
    "        # Existing plots\n",
    "        plot_predictions(\n",
    "            predictions, actuals, time_indices[sort_idx], rmse_phi, rmse_theta,\n",
    "            f'predictions_model_{model_name}.png',\n",
    "            f'Model {model_name.capitalize()} (Window Size {window_size})'\n",
    "        )\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'rmse_phi': rmse_phi,\n",
    "            'rmse_theta': rmse_theta\n",
    "        }\n",
    "\n",
    "        # Loss Curve Plot\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.title(f'Loss Curves (Model: {model_name}, Window Size: {window_size})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_model_{model_name}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot RMSE Comparison Chart\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x - width/2, rmse_phi_list, width, label='RMSE Phi')\n",
    "    plt.bar(x + width/2, rmse_theta_list, width, label='RMSE Theta')\n",
    "    plt.xticks(x, model_names)\n",
    "    plt.ylabel('RMSE (degrees)')\n",
    "    plt.title('RMSE Comparison Across Model Sizes')\n",
    "    plt.legend()\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rmse_model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = os.path.join(\"..\", \"Data\", \"data_angles\", \"txp_1551.5_pax_1552.5_polcon_and_fiber.csv\")\n",
    "    \n",
    "    print(\"Attempting to access:\", data_path)\n",
    "    print(\"Full path:\", os.path.abspath(data_path))\n",
    "    if os.path.exists(data_path):\n",
    "        print(\"File found!\")\n",
    "    else:\n",
    "        print(\"File not found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Experiment 1: Window sizes\n",
    "    window_sizes = [8, 16, 32, 64, 128]\n",
    "    window_results = experiment_window_sizes(data_path, window_sizes)\n",
    "    \n",
    "    # Experiment 2: Model sizes for window_size=64\n",
    "    model_results = experiment_model_sizes(data_path, window_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d391e2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e37c542-d378-4213-94ed-ec22d420e181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['small', 'medium', 'large', 'extra_large'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc485a19-322b-490f-8e19-e43ca9257d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([{'train_losses': [0.3046268061284096, 0.059922470320617, 0.050961285408946774, 0.04874727024357715, 0.047820143178448696, 0.04685221828760639, 0.04628581955410918, 0.045867144761066285, 0.045256047316479246, 0.044980418992825365, 0.04464713978465252, 0.04439954973825936, 0.04450364913018892, 0.04412431441556473, 0.04382363696526822, 0.04395231777106837, 0.04385914647531125, 0.043566210701855644, 0.043666298170724226, 0.04347203850471479, 0.043379977620142396, 0.043147739709659655, 0.04314233407309528, 0.04306119988556557, 0.04300235187815082, 0.043051512602905524, 0.04300821007740113, 0.04278913753167275, 0.04285919325520641, 0.04290605048876479, 0.04278279820345514, 0.042778160749217885, 0.04281611182653959, 0.04273255789987228, 0.04277592694841772, 0.0426725155249032, 0.042697470120158613, 0.04249512955271703, 0.042754136952936374, 0.04257642683757615, 0.04268607945280141, 0.04259936221390276, 0.04268081143738762, 0.042598370505574114, 0.04259681360890514, 0.04244740666126326, 0.04261505423534301, 0.04251780502566819, 0.04266790711762993, 0.04267326936400431], 'test_losses': [0.09455564706162974, 0.07439843074164608, 0.08066156608137218, 0.07536680095575073, 0.07957974645224485, 0.08282775983891703, 0.08101573173295368, 0.07865754457359964, 0.07766253264112906, 0.08647888292643156, 0.08049548241225156, 0.08316671858457002, 0.07644472406669096, 0.08195252367718653, 0.08553329055959528, 0.08768563135103746, 0.07870826575566423, 0.0858706433326006, 0.08308296833526005, 0.08259975005957214, 0.08491176634349606, 0.08380818712440403, 0.08694675392047925, 0.08287261175838384, 0.08864676874469626, 0.08396939923140136, 0.08052545975555073, 0.08305611749264327, 0.08204832398755983, 0.0831246520307931, 0.0781301378526471, 0.0811986475505612, 0.08334453793411906, 0.08222183348103003, 0.08208430636335504, 0.08384793898598715, 0.08386256325651299, 0.08459118591113524, 0.08317967368797823, 0.08572045141323047, 0.08492238304831765, 0.08488302366300063, 0.08415380489419808, 0.08523233858021823, 0.08484251346777785, 0.08471473313190721, 0.08482540808618069, 0.08481601899997755, 0.08482102358883077, 0.08483356193385341], 'rmse_phi': 64.23965, 'rmse_theta': 36.80852}, {'train_losses': [0.12663100152555423, 0.057605115456446525, 0.05211701609777965, 0.048632526881821146, 0.04697327320957514, 0.0460777704943023, 0.04539538875290875, 0.044340350420518954, 0.043646272632383534, 0.043573540957292656, 0.04292919855856676, 0.042745148591388206, 0.04214165735697966, 0.04183383007031707, 0.04132456550469047, 0.04066448189180842, 0.040225716324926525, 0.03975617901898474, 0.03933843280175864, 0.03916782533861525, 0.0390187233240099, 0.03870998662874995, 0.03820491135807081, 0.03813052344809754, 0.03778751062289361, 0.03744811333784585, 0.037267207112249144, 0.03697606077426315, 0.03679026059875016, 0.03674061771499396, 0.03647974803872098, 0.036545951769167924, 0.03627739710226861, 0.036274851973636356, 0.03617409352333315, 0.03582880082905018, 0.03581740175833076, 0.035731919573336705, 0.03585512027110098, 0.03562112985187412, 0.03582450595559887, 0.0356202193274064, 0.0356577340307461, 0.03564261197872151, 0.035429717526526495, 0.03538431905313021, 0.035377815056781066, 0.03565964379137562, 0.035449055800308826, 0.035592798684393204], 'test_losses': [0.10334459311244162, 0.0826009753075513, 0.09249127795073118, 0.0790571042759852, 0.07935558452524923, 0.08063266296786341, 0.07247234051200477, 0.07571057491004467, 0.08597020276568153, 0.0807682312686335, 0.08721841638061133, 0.07645283624191176, 0.0910193514756181, 0.08668275378983128, 0.084058129042387, 0.08332978681745855, 0.0836865279992873, 0.08644512476907536, 0.0959975390271707, 0.08288999155841091, 0.08244062909348444, 0.08659307943149046, 0.08465033182027665, 0.08450523958626119, 0.08880526230416515, 0.08890102641149, 0.08977732135152275, 0.08623864765871656, 0.08974441015584902, 0.0841052065349438, 0.08722816041924737, 0.08607296704907309, 0.08443226651711898, 0.0918486209755594, 0.08825987362387505, 0.08584889161654494, 0.08806896660138261, 0.08962456072595987, 0.08737493021921679, 0.08520990471270952, 0.08619952675971118, 0.08845339312472127, 0.0887237592854283, 0.08417417818511075, 0.08667670281773264, 0.08503940781070428, 0.08604671745137735, 0.08602171621539376, 0.08602960592305119, 0.08617789743637497], 'rmse_phi': 62.434017, 'rmse_theta': 41.03379}, {'train_losses': [0.08912754928358414, 0.04920893510983836, 0.045484944972025085, 0.044088029478620824, 0.04216642279504081, 0.04016449498141416, 0.03851638604728033, 0.03737112684046618, 0.03699252190506128, 0.03635464199230693, 0.03608246500419307, 0.03602769387207822, 0.035441960100822734, 0.03503377832609662, 0.034622906402508785, 0.03397126450344989, 0.03384749325258391, 0.0335039759166367, 0.033126606181057915, 0.03258171818360755, 0.03220118063798148, 0.03196076685691484, 0.03161032922771944, 0.031513383620620325, 0.03129775809287201, 0.031179639703941785, 0.030795730315663846, 0.030532579431267377, 0.030650340836100316, 0.030502248714218767, 0.03044873072598387, 0.030128010788992528, 0.030111352582612345, 0.030081612628794486, 0.029934942627900755, 0.029929348057316195, 0.029828918565596853, 0.029739294879241474, 0.029641652874614236, 0.029601137939205366, 0.02972229247835512, 0.029585691778341198, 0.029410695717219385, 0.02931568900354996, 0.02933505914830667, 0.02917812496835735, 0.029305720521557714, 0.02937503320418195, 0.02940050584654654, 0.02924635818374047], 'test_losses': [0.07589553025635806, 0.10548124767162582, 0.09192564898932522, 0.07451874032955277, 0.10228249000554734, 0.07241972066800703, 0.0873869251459837, 0.09644861133261161, 0.08358772229403258, 0.1091524751348929, 0.1147366061468016, 0.09175103448162025, 0.09588983588936655, 0.0799542761831121, 0.0928899169137532, 0.09197252864356745, 0.102665695971386, 0.09569861430844123, 0.09446748289364305, 0.11000686298039826, 0.09829415253109552, 0.09743803128261458, 0.10006586877967824, 0.10320464006879113, 0.0964878346432339, 0.10077373719012195, 0.09467022871090607, 0.096883110570806, 0.10095682706345212, 0.09602950125594031, 0.09572343088855798, 0.09128606983206489, 0.08233118593184785, 0.09285901924171909, 0.09612941637381234, 0.08792164420539683, 0.09143711778504604, 0.10527308156544513, 0.09262108735909516, 0.09454673093489625, 0.09400959613478997, 0.09500754869712347, 0.08874957282519476, 0.09038476539691064, 0.09410345188596032, 0.09429318884556943, 0.09392290798608552, 0.09393726120787588, 0.0928714380430227, 0.09303840708664872], 'rmse_phi': 64.10637, 'rmse_theta': 44.309532}, {'train_losses': [0.14441238006379473, 0.04700877902866234, 0.04347637260530127, 0.040052827035639144, 0.0397181553843384, 0.03765681255522961, 0.036890457689006756, 0.036533377532448084, 0.034997783472529756, 0.034753009402257506, 0.03381395985453909, 0.03326928350336266, 0.0327965708423732, 0.03206490511546761, 0.031088760749260952, 0.030240567710061776, 0.02974451495514762, 0.029163379677086383, 0.02925662051302634, 0.028919966569522285, 0.02882865314213087, 0.02816404064061455, 0.028280114003967856, 0.028021404978900735, 0.02777935946488985, 0.027445492381873767, 0.02733201145576442, 0.02740014395097159, 0.0271741319533592, 0.02693974562332652, 0.026935996616490977, 0.026911875160093406, 0.026603188910757615, 0.02650586260302413, 0.026400716554734013, 0.026330518454725293, 0.026565327974302427, 0.026392124571798858, 0.026068982479381397, 0.02592955962733327, 0.025895857410977514, 0.025936859084748177, 0.02585344185220069, 0.025814460564730902, 0.025771777733542403, 0.025774760077351248, 0.02557524018985335, 0.02547833094296093, 0.02559133629799576, 0.02562991287645107], 'test_losses': [0.106018365458162, 0.12282365657050501, 0.07606505311348222, 0.08222813516516578, 0.08242036602036519, 0.11283625218678604, 0.07530733490870757, 0.08395213701508263, 0.10239768559959801, 0.11082438467578455, 0.11265004534613002, 0.07888145678761331, 0.08632355133410205, 0.08705146532421085, 0.1088792535053058, 0.08049241768788885, 0.0843910387547856, 0.087111398531124, 0.08642505977768451, 0.0834648775275458, 0.10357723826203834, 0.08927100422707471, 0.0926164951260117, 0.09813980154523795, 0.07557074571536346, 0.0930186021649702, 0.09159394019198688, 0.07166015895676206, 0.08136617551473053, 0.09126930724490773, 0.0868524851192805, 0.08484070158817551, 0.08564806287176907, 0.09159278029745276, 0.08538364545357498, 0.09235924921760505, 0.08548092434013432, 0.09217166865921833, 0.08821083255620166, 0.08900494752451778, 0.08655782547694715, 0.09283166572620923, 0.08972549994730136, 0.09091740591790189, 0.08776796957985922, 0.08915017335252329, 0.09061474714597517, 0.08930391781878742, 0.08947576164183292, 0.08964860402047634], 'rmse_phi': 60.59071, 'rmse_theta': 46.965168}])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ef569-84b3-45fe-9d97-50df7ffa6b89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
