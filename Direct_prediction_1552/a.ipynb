{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ac3c0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to access: ../Data/data_angles/data4_angles.csv\n",
      "Full path: /home/exx/Desktop/quantum/Data/data_angles/data4_angles.csv\n",
      "File found!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "data_path = \"../Data/data_angles/data4_angles.csv\"\n",
    "print(\"Attempting to access:\", data_path)\n",
    "print(\"Full path:\", os.path.abspath(data_path))\n",
    "if os.path.exists(data_path):\n",
    "    print(\"File found!\")\n",
    "else:\n",
    "    print(\"File not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c16cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 12.4\n",
      "GPU count: 1\n",
      "GPU name: NVIDIA H100 NVL\n",
      "Attempting to access: ../Data/data_angles/data4_angles.csv\n",
      "Full path: /home/exx/Desktop/quantum/Data/data_angles/data4_angles.csv\n",
      "File found!\n",
      "\n",
      "Experimenting with window size: 8\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/quantum/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 3.2362, Test Loss: 3.7441\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 3.2469, Test Loss: 3.7441\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 3.0997, Test Loss: 3.4388\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 2.8561, Test Loss: 3.1443\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 2.5638, Test Loss: 2.8668\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 2.3525, Test Loss: 2.6041\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 2.1144, Test Loss: 2.3565\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 1.8132, Test Loss: 2.1256\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 1.6864, Test Loss: 1.9126\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 1.4867, Test Loss: 1.7171\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 1.2844, Test Loss: 1.5391\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 1.1673, Test Loss: 1.3789\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 1.0109, Test Loss: 1.2350\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.8879, Test Loss: 1.1076\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.8038, Test Loss: 0.9979\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.6899, Test Loss: 0.9020\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.6356, Test Loss: 0.8206\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.6019, Test Loss: 0.7544\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.5512, Test Loss: 0.7005\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.5325, Test Loss: 0.6565\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.4745, Test Loss: 0.6215\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.4525, Test Loss: 0.5959\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.4740, Test Loss: 0.5762\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.4393, Test Loss: 0.5604\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.4283, Test Loss: 0.5479\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.4135, Test Loss: 0.5368\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.4083, Test Loss: 0.5276\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.3681, Test Loss: 0.5200\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.4032, Test Loss: 0.5127\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.3925, Test Loss: 0.5057\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.3600, Test Loss: 0.4992\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.3808, Test Loss: 0.4927\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.3615, Test Loss: 0.4872\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.3211, Test Loss: 0.4815\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.3695, Test Loss: 0.4766\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.3690, Test Loss: 0.4722\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.3505, Test Loss: 0.4689\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.3293, Test Loss: 0.4658\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.3275, Test Loss: 0.4631\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.3035, Test Loss: 0.4610\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.3530, Test Loss: 0.4591\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.3475, Test Loss: 0.4575\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.3356, Test Loss: 0.4565\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.3434, Test Loss: 0.4555\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.3361, Test Loss: 0.4550\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.3286, Test Loss: 0.4545\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.3093, Test Loss: 0.4543\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.3299, Test Loss: 0.4542\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.3064, Test Loss: 0.4541\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.3282, Test Loss: 0.4540\n",
      "\n",
      "Experimenting with window size: 16\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 1.8292, Test Loss: 2.0180\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 1.7995, Test Loss: 1.8996\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 1.6276, Test Loss: 1.6831\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 1.4441, Test Loss: 1.4849\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 1.3281, Test Loss: 1.3072\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 1.1376, Test Loss: 1.1506\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.9956, Test Loss: 1.0093\n",
      "Batch x device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/quantum/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.8945, Test Loss: 0.8894\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.8008, Test Loss: 0.7878\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.7614, Test Loss: 0.7034\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.6523, Test Loss: 0.6330\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.6099, Test Loss: 0.5789\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.5860, Test Loss: 0.5385\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.5391, Test Loss: 0.5096\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.5589, Test Loss: 0.4886\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.5443, Test Loss: 0.4744\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.5131, Test Loss: 0.4643\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.4986, Test Loss: 0.4550\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.4927, Test Loss: 0.4450\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.4632, Test Loss: 0.4345\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.4650, Test Loss: 0.4255\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.4657, Test Loss: 0.4154\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.4445, Test Loss: 0.4067\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.4358, Test Loss: 0.3992\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.4235, Test Loss: 0.3926\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.4215, Test Loss: 0.3879\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.4292, Test Loss: 0.3828\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.4145, Test Loss: 0.3785\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.4072, Test Loss: 0.3735\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.3915, Test Loss: 0.3688\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.4011, Test Loss: 0.3655\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.4052, Test Loss: 0.3617\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.3978, Test Loss: 0.3585\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.3965, Test Loss: 0.3564\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.3629, Test Loss: 0.3539\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.3938, Test Loss: 0.3514\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.3833, Test Loss: 0.3493\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.3840, Test Loss: 0.3474\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.3912, Test Loss: 0.3463\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.3989, Test Loss: 0.3452\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.3575, Test Loss: 0.3444\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.3752, Test Loss: 0.3437\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.3796, Test Loss: 0.3431\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.3833, Test Loss: 0.3426\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.3574, Test Loss: 0.3424\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.3754, Test Loss: 0.3421\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.3755, Test Loss: 0.3418\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.3620, Test Loss: 0.3418\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.3771, Test Loss: 0.3418\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.3699, Test Loss: 0.3417\n",
      "\n",
      "Experimenting with window size: 32\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 3.2141, Test Loss: 3.1273\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 3.1985, Test Loss: 3.0530\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 2.9868, Test Loss: 2.9073\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 2.9348, Test Loss: 2.7708\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 2.8079, Test Loss: 2.6477\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 2.6499, Test Loss: 2.5379\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 2.5743, Test Loss: 2.4445\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 2.4776, Test Loss: 2.3674\n",
      "Batch x device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/quantum/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 2.3672, Test Loss: 2.2973\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 2.2100, Test Loss: 2.2370\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 2.1836, Test Loss: 2.1787\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 2.1823, Test Loss: 2.1238\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 2.0966, Test Loss: 2.0747\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 2.0794, Test Loss: 2.0259\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 2.0108, Test Loss: 1.9817\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 1.9016, Test Loss: 1.9360\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 1.9021, Test Loss: 1.8939\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 1.8401, Test Loss: 1.8542\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 1.8233, Test Loss: 1.8135\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 1.8161, Test Loss: 1.7769\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 1.7253, Test Loss: 1.7423\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 1.6849, Test Loss: 1.7083\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 1.6763, Test Loss: 1.6752\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 1.6475, Test Loss: 1.6438\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 1.5784, Test Loss: 1.6142\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 1.6611, Test Loss: 1.5861\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 1.4832, Test Loss: 1.5616\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 1.5747, Test Loss: 1.5369\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 1.6633, Test Loss: 1.5172\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 1.5212, Test Loss: 1.4980\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 1.5186, Test Loss: 1.4801\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 1.5433, Test Loss: 1.4646\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 1.4877, Test Loss: 1.4513\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 1.3992, Test Loss: 1.4393\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 1.4040, Test Loss: 1.4283\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 1.5284, Test Loss: 1.4183\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 1.4835, Test Loss: 1.4115\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 1.4113, Test Loss: 1.4051\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 1.4920, Test Loss: 1.3991\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 1.5230, Test Loss: 1.3946\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 1.4320, Test Loss: 1.3906\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 1.4493, Test Loss: 1.3877\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 1.4728, Test Loss: 1.3848\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 1.4184, Test Loss: 1.3827\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 1.4379, Test Loss: 1.3813\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 1.4249, Test Loss: 1.3803\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 1.3704, Test Loss: 1.3796\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 1.4025, Test Loss: 1.3789\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 1.3609, Test Loss: 1.3790\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 1.4105, Test Loss: 1.3788\n",
      "\n",
      "Experimenting with window size: 64\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 4.1056, Test Loss: 4.2741\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 4.1508, Test Loss: 4.2741\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 2.9737, Test Loss: 3.9853\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 3.6763, Test Loss: 3.7212\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 3.3572, Test Loss: 3.4437\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 2.3581, Test Loss: 3.1732\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 2.3476, Test Loss: 2.9213\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 2.9402, Test Loss: 2.6888\n",
      "Batch x device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/quantum/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 2.2546, Test Loss: 2.4739\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 2.0418, Test Loss: 2.2797\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 1.4935, Test Loss: 2.1079\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 2.0103, Test Loss: 1.9553\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 1.9073, Test Loss: 1.8311\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 1.4458, Test Loss: 1.7300\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 1.7816, Test Loss: 1.6367\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 1.5037, Test Loss: 1.5544\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 1.6104, Test Loss: 1.4825\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 1.7579, Test Loss: 1.4199\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 1.4528, Test Loss: 1.3704\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 1.1209, Test Loss: 1.3299\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 1.0927, Test Loss: 1.2935\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 1.4909, Test Loss: 1.2617\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 1.5422, Test Loss: 1.2333\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 1.5810, Test Loss: 1.2085\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 1.2812, Test Loss: 1.1850\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 1.1358, Test Loss: 1.1661\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 1.0343, Test Loss: 1.1508\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 1.5081, Test Loss: 1.1359\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 1.4063, Test Loss: 1.1243\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 1.4079, Test Loss: 1.1134\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 1.2588, Test Loss: 1.1043\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 1.6567, Test Loss: 1.0970\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 1.4577, Test Loss: 1.0914\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 1.1383, Test Loss: 1.0863\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 1.1183, Test Loss: 1.0821\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 1.4998, Test Loss: 1.0783\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 1.5578, Test Loss: 1.0753\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 1.2266, Test Loss: 1.0725\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 1.3087, Test Loss: 1.0714\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 1.3976, Test Loss: 1.0694\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 1.2942, Test Loss: 1.0688\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 1.2130, Test Loss: 1.0673\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 1.4570, Test Loss: 1.0664\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 1.1252, Test Loss: 1.0659\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.9997, Test Loss: 1.0648\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 1.2068, Test Loss: 1.0638\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 1.2365, Test Loss: 1.0636\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 1.1738, Test Loss: 1.0631\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 1.4292, Test Loss: 1.0635\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 1.2438, Test Loss: 1.0635\n",
      "\n",
      "Experimenting with window size: 128\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 1.7376, Test Loss: 1.6805\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 1.7161, Test Loss: 1.6805\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 1.7215, Test Loss: 1.6805\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 1.7084, Test Loss: 1.5873\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 1.6287, Test Loss: 1.4973\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 1.5272, Test Loss: 1.4122\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 1.4457, Test Loss: 1.3336\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 1.3617, Test Loss: 1.2588\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 1.2802, Test Loss: 1.1912\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 1.2118, Test Loss: 1.1266\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 1.1405, Test Loss: 1.0672\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 1.0605, Test Loss: 1.0124\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.9901, Test Loss: 0.9638\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.9473, Test Loss: 0.9187\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.9131, Test Loss: 0.8771\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.8499, Test Loss: 0.8406\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.8009, Test Loss: 0.8071\n",
      "Batch x device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/quantum/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.7694, Test Loss: 0.7771\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.7487, Test Loss: 0.7512\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.6997, Test Loss: 0.7281\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.6927, Test Loss: 0.7074\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.6604, Test Loss: 0.6896\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.6348, Test Loss: 0.6734\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.6077, Test Loss: 0.6599\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.5991, Test Loss: 0.6483\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.5679, Test Loss: 0.6381\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.5569, Test Loss: 0.6291\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.5473, Test Loss: 0.6222\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.5266, Test Loss: 0.6166\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.5261, Test Loss: 0.6109\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.5330, Test Loss: 0.6067\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.5129, Test Loss: 0.6024\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.5007, Test Loss: 0.5996\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.4931, Test Loss: 0.5968\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.4928, Test Loss: 0.5945\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.5059, Test Loss: 0.5927\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.5017, Test Loss: 0.5917\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.4875, Test Loss: 0.5907\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.4889, Test Loss: 0.5887\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.4806, Test Loss: 0.5879\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.4873, Test Loss: 0.5876\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.4683, Test Loss: 0.5866\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.4812, Test Loss: 0.5866\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.4765, Test Loss: 0.5860\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.4811, Test Loss: 0.5863\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.4790, Test Loss: 0.5862\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.4676, Test Loss: 0.5856\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.4762, Test Loss: 0.5855\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.4844, Test Loss: 0.5855\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.4654, Test Loss: 0.5854\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "Normalized Feature Phi range: -0.9835503548102013 to 0.991698146715186\n",
      "Normalized Feature Theta range: 0.05437736252287956 to 0.9571765560711043\n",
      "Normalized Target Phi range: -0.994472659092212 to 0.9998890201155154\n",
      "Normalized Target Theta range: 0.027904612870393794 to 0.9482130420030769\n",
      "\n",
      "Experimenting with model size: small (d_model=16, n_layers=1, n_heads=4)\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 0.9114, Test Loss: 1.2072\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 0.6862, Test Loss: 1.2005\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.7809, Test Loss: 1.1615\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.7661, Test Loss: 1.1171\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.7857, Test Loss: 1.0764\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.8878, Test Loss: 1.0576\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.8258, Test Loss: 1.0191\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.5028, Test Loss: 0.9849\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.7290, Test Loss: 0.9529\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.6618, Test Loss: 0.9195\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.6548, Test Loss: 0.8861\n",
      "Batch x device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exx/anaconda3/envs/quantum/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.6225, Test Loss: 0.8546\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.7480, Test Loss: 0.8227\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.7174, Test Loss: 0.7921\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.6301, Test Loss: 0.7636\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.5623, Test Loss: 0.7369\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.5483, Test Loss: 0.7127\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.6046, Test Loss: 0.6897\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.6101, Test Loss: 0.6672\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.4775, Test Loss: 0.6474\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.4877, Test Loss: 0.6285\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.4852, Test Loss: 0.6098\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.4478, Test Loss: 0.5941\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.4518, Test Loss: 0.5812\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.4222, Test Loss: 0.5690\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.4806, Test Loss: 0.5585\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.3980, Test Loss: 0.5491\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.5401, Test Loss: 0.5417\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.4059, Test Loss: 0.5340\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.4471, Test Loss: 0.5261\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.4217, Test Loss: 0.5195\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.4384, Test Loss: 0.5134\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.4474, Test Loss: 0.5071\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.4544, Test Loss: 0.5021\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.3598, Test Loss: 0.4973\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.3864, Test Loss: 0.4938\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.3530, Test Loss: 0.4913\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.4415, Test Loss: 0.4882\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.4735, Test Loss: 0.4864\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.4173, Test Loss: 0.4845\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.4034, Test Loss: 0.4830\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.3768, Test Loss: 0.4812\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.4176, Test Loss: 0.4800\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.4885, Test Loss: 0.4792\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.4337, Test Loss: 0.4790\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.4034, Test Loss: 0.4786\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.4851, Test Loss: 0.4784\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.3918, Test Loss: 0.4782\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.3439, Test Loss: 0.4781\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.4409, Test Loss: 0.4781\n",
      "\n",
      "Experimenting with model size: medium (d_model=32, n_layers=2, n_heads=4)\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 3.3287, Test Loss: 3.6014\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 3.5207, Test Loss: 3.6014\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 3.7360, Test Loss: 3.3211\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 3.5362, Test Loss: 3.0567\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 3.3279, Test Loss: 2.8056\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 2.9558, Test Loss: 2.5745\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 2.7589, Test Loss: 2.3574\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 2.2656, Test Loss: 2.1560\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 1.9105, Test Loss: 1.9700\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 2.1031, Test Loss: 1.7999\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 1.7248, Test Loss: 1.6426\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 1.7325, Test Loss: 1.5009\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 1.3817, Test Loss: 1.3736\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 1.4848, Test Loss: 1.2600\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 1.0621, Test Loss: 1.1595\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 1.1995, Test Loss: 1.0726\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 1.3067, Test Loss: 0.9968\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 1.1945, Test Loss: 0.9332\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 1.0817, Test Loss: 0.8840\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 1.2053, Test Loss: 0.8419\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 1.0855, Test Loss: 0.8066\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 1.1605, Test Loss: 0.7763\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.8494, Test Loss: 0.7490\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 1.0669, Test Loss: 0.7244\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.7587, Test Loss: 0.7011\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 1.0132, Test Loss: 0.6799\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.6455, Test Loss: 0.6615\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.8785, Test Loss: 0.6458\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 1.0129, Test Loss: 0.6314\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.6708, Test Loss: 0.6177\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.9663, Test Loss: 0.6066\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.5911, Test Loss: 0.5972\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.8936, Test Loss: 0.5896\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.8416, Test Loss: 0.5824\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.7425, Test Loss: 0.5767\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.6703, Test Loss: 0.5709\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.7756, Test Loss: 0.5658\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.7646, Test Loss: 0.5617\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.6997, Test Loss: 0.5584\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.6720, Test Loss: 0.5563\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.7275, Test Loss: 0.5545\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.5984, Test Loss: 0.5531\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 1.0091, Test Loss: 0.5522\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.7585, Test Loss: 0.5511\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.7868, Test Loss: 0.5505\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.6245, Test Loss: 0.5500\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.5486, Test Loss: 0.5495\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.7196, Test Loss: 0.5493\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.7172, Test Loss: 0.5495\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.8262, Test Loss: 0.5494\n",
      "\n",
      "Experimenting with model size: large (d_model=64, n_layers=3, n_heads=8)\n",
      "Model device: cuda:0\n",
      "Batch x device: cuda:0\n",
      "Epoch 1/50, Train Loss: 1.5250, Test Loss: 1.6431\n",
      "Batch x device: cuda:0\n",
      "Epoch 2/50, Train Loss: 1.1853, Test Loss: 1.3036\n",
      "Batch x device: cuda:0\n",
      "Epoch 3/50, Train Loss: 0.9243, Test Loss: 0.7988\n",
      "Batch x device: cuda:0\n",
      "Epoch 4/50, Train Loss: 0.5775, Test Loss: 0.5023\n",
      "Batch x device: cuda:0\n",
      "Epoch 5/50, Train Loss: 0.5038, Test Loss: 0.3620\n",
      "Batch x device: cuda:0\n",
      "Epoch 6/50, Train Loss: 0.4133, Test Loss: 0.3368\n",
      "Batch x device: cuda:0\n",
      "Epoch 7/50, Train Loss: 0.5377, Test Loss: 0.3321\n",
      "Batch x device: cuda:0\n",
      "Epoch 8/50, Train Loss: 0.4511, Test Loss: 0.3087\n",
      "Batch x device: cuda:0\n",
      "Epoch 9/50, Train Loss: 0.4145, Test Loss: 0.2811\n",
      "Batch x device: cuda:0\n",
      "Epoch 10/50, Train Loss: 0.3938, Test Loss: 0.2681\n",
      "Batch x device: cuda:0\n",
      "Epoch 11/50, Train Loss: 0.3292, Test Loss: 0.2675\n",
      "Batch x device: cuda:0\n",
      "Epoch 12/50, Train Loss: 0.3089, Test Loss: 0.2627\n",
      "Batch x device: cuda:0\n",
      "Epoch 13/50, Train Loss: 0.3131, Test Loss: 0.2437\n",
      "Batch x device: cuda:0\n",
      "Epoch 14/50, Train Loss: 0.2932, Test Loss: 0.2225\n",
      "Batch x device: cuda:0\n",
      "Epoch 15/50, Train Loss: 0.2697, Test Loss: 0.2070\n",
      "Batch x device: cuda:0\n",
      "Epoch 16/50, Train Loss: 0.2618, Test Loss: 0.1982\n",
      "Batch x device: cuda:0\n",
      "Epoch 17/50, Train Loss: 0.2404, Test Loss: 0.1936\n",
      "Batch x device: cuda:0\n",
      "Epoch 18/50, Train Loss: 0.2507, Test Loss: 0.1952\n",
      "Batch x device: cuda:0\n",
      "Epoch 19/50, Train Loss: 0.2122, Test Loss: 0.1941\n",
      "Batch x device: cuda:0\n",
      "Epoch 20/50, Train Loss: 0.1902, Test Loss: 0.1857\n",
      "Batch x device: cuda:0\n",
      "Epoch 21/50, Train Loss: 0.1705, Test Loss: 0.1795\n",
      "Batch x device: cuda:0\n",
      "Epoch 22/50, Train Loss: 0.1700, Test Loss: 0.1760\n",
      "Batch x device: cuda:0\n",
      "Epoch 23/50, Train Loss: 0.2234, Test Loss: 0.1703\n",
      "Batch x device: cuda:0\n",
      "Epoch 24/50, Train Loss: 0.1907, Test Loss: 0.1610\n",
      "Batch x device: cuda:0\n",
      "Epoch 25/50, Train Loss: 0.1535, Test Loss: 0.1545\n",
      "Batch x device: cuda:0\n",
      "Epoch 26/50, Train Loss: 0.1170, Test Loss: 0.1516\n",
      "Batch x device: cuda:0\n",
      "Epoch 27/50, Train Loss: 0.2180, Test Loss: 0.1498\n",
      "Batch x device: cuda:0\n",
      "Epoch 28/50, Train Loss: 0.1982, Test Loss: 0.1498\n",
      "Batch x device: cuda:0\n",
      "Epoch 29/50, Train Loss: 0.1530, Test Loss: 0.1489\n",
      "Batch x device: cuda:0\n",
      "Epoch 30/50, Train Loss: 0.1431, Test Loss: 0.1492\n",
      "Batch x device: cuda:0\n",
      "Epoch 31/50, Train Loss: 0.1377, Test Loss: 0.1486\n",
      "Batch x device: cuda:0\n",
      "Epoch 32/50, Train Loss: 0.1010, Test Loss: 0.1465\n",
      "Batch x device: cuda:0\n",
      "Epoch 33/50, Train Loss: 0.1086, Test Loss: 0.1436\n",
      "Batch x device: cuda:0\n",
      "Epoch 34/50, Train Loss: 0.1412, Test Loss: 0.1413\n",
      "Batch x device: cuda:0\n",
      "Epoch 35/50, Train Loss: 0.1091, Test Loss: 0.1395\n",
      "Batch x device: cuda:0\n",
      "Epoch 36/50, Train Loss: 0.1100, Test Loss: 0.1386\n",
      "Batch x device: cuda:0\n",
      "Epoch 37/50, Train Loss: 0.1406, Test Loss: 0.1385\n",
      "Batch x device: cuda:0\n",
      "Epoch 38/50, Train Loss: 0.1385, Test Loss: 0.1380\n",
      "Batch x device: cuda:0\n",
      "Epoch 39/50, Train Loss: 0.1519, Test Loss: 0.1378\n",
      "Batch x device: cuda:0\n",
      "Epoch 40/50, Train Loss: 0.1639, Test Loss: 0.1370\n",
      "Batch x device: cuda:0\n",
      "Epoch 41/50, Train Loss: 0.1774, Test Loss: 0.1360\n",
      "Batch x device: cuda:0\n",
      "Epoch 42/50, Train Loss: 0.1587, Test Loss: 0.1354\n",
      "Batch x device: cuda:0\n",
      "Epoch 43/50, Train Loss: 0.1372, Test Loss: 0.1351\n",
      "Batch x device: cuda:0\n",
      "Epoch 44/50, Train Loss: 0.1214, Test Loss: 0.1353\n",
      "Batch x device: cuda:0\n",
      "Epoch 45/50, Train Loss: 0.0957, Test Loss: 0.1354\n",
      "Batch x device: cuda:0\n",
      "Epoch 46/50, Train Loss: 0.1507, Test Loss: 0.1357\n",
      "Batch x device: cuda:0\n",
      "Epoch 47/50, Train Loss: 0.1140, Test Loss: 0.1358\n",
      "Batch x device: cuda:0\n",
      "Epoch 48/50, Train Loss: 0.1087, Test Loss: 0.1358\n",
      "Batch x device: cuda:0\n",
      "Epoch 49/50, Train Loss: 0.1059, Test Loss: 0.1358\n",
      "Batch x device: cuda:0\n",
      "Epoch 50/50, Train Loss: 0.1344, Test Loss: 0.1357\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import GradScaler, autocast\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# Verify GPU\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU detected, exiting!\")\n",
    "    exit(1)\n",
    "\n",
    "class AngleDataset(Dataset):\n",
    "    def __init__(self, data_path: str, window_size: int, indices: np.ndarray = None):\n",
    "        self.df = pd.read_csv(data_path)\n",
    "        if self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                   'Target Phi (degrees)', 'Target Theta (degrees)']].isna().any().any():\n",
    "            print(\"Warning: NaN values found in data!\")\n",
    "            self.df = self.df.dropna()\n",
    "        if np.isinf(self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                            'Target Phi (degrees)', 'Target Theta (degrees)']].values).any():\n",
    "            print(\"Warning: Infinite values found in data!\")\n",
    "            self.df = self.df[~np.isinf(self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)', \n",
    "                                                'Target Phi (degrees)', 'Target Theta (degrees)']]).any(axis=1)]\n",
    "        \n",
    "        self.features = self.df[['Feature Phi (degrees)', 'Feature Theta (degrees)']].values\n",
    "        self.targets = self.df[['Target Phi (degrees)', 'Target Theta (degrees)']].values\n",
    "        self.features[:, 0] /= 180.0\n",
    "        self.features[:, 1] /= 180.0\n",
    "        self.targets[:, 0] /= 180.0\n",
    "        self.targets[:, 1] /= 180.0\n",
    "        \n",
    "        print(\"Normalized Feature Phi range:\", self.features[:, 0].min(), \"to\", self.features[:, 0].max())\n",
    "        print(\"Normalized Feature Theta range:\", self.features[:, 1].min(), \"to\", self.features[:, 1].max())\n",
    "        print(\"Normalized Target Phi range:\", self.targets[:, 0].min(), \"to\", self.targets[:, 0].max())\n",
    "        print(\"Normalized Target Theta range:\", self.targets[:, 1].min(), \"to\", self.targets[:, 1].max())\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.indices = indices if indices is not None else np.arange(len(self.df) - window_size + 1)\n",
    "        self.length = len(self.indices)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
    "        data_idx = self.indices[idx]\n",
    "        window = self.features[data_idx:data_idx + self.window_size]\n",
    "        target = self.targets[data_idx + self.window_size - 1]\n",
    "        time_index = data_idx + self.window_size - 1\n",
    "        return torch.FloatTensor(window), torch.FloatTensor(target), time_index\n",
    "\n",
    "class FlashAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, T, 3, self.n_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        scale = 1.0 / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        \n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).reshape(B, T, C)\n",
    "        out = self.out(out)\n",
    "        return out\n",
    "\n",
    "class AnglePredictionModel(nn.Module):\n",
    "    def __init__(self, input_dim: int, d_model: int, n_heads: int, n_layers: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 128, d_model) * 0.1)\n",
    "        self.attn_layers = nn.ModuleList([\n",
    "            FlashAttention(d_model, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.norm_layers = nn.ModuleList([\n",
    "            nn.LayerNorm(d_model) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(d_model, 2)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.input_proj(x)\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :]\n",
    "        \n",
    "        for attn, norm in zip(self.attn_layers, self.norm_layers):\n",
    "            residual = x\n",
    "            x = attn(x)\n",
    "            x = norm(x + residual)\n",
    "        \n",
    "        x = x[:, -1, :]\n",
    "        return self.output(x)\n",
    "\n",
    "def train_model(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader, \n",
    "                epochs: int, device: torch.device, lr: float = 1e-4):\n",
    "    print(\"Model device:\", next(model.parameters()).device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for i, (batch_x, batch_y, _) in enumerate(train_loader):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            if i == 0:\n",
    "                print(\"Batch x device:\", batch_x.device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                output = model(batch_x)\n",
    "                if torch.isnan(output).any():\n",
    "                    print(f\"NaN in output at epoch {epoch+1}, batch {i}\")\n",
    "                    break\n",
    "                loss = criterion(output, batch_y)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, _ in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                test_loss += criterion(output, batch_y).item()\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses\n",
    "\n",
    "def plot_predictions(predictions: np.ndarray, actuals: np.ndarray, time_indices: np.ndarray, \n",
    "                     rmse_phi: float, rmse_theta: float, filename: str, title_prefix: str):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(time_indices, actuals[:, 0], label='Actual Phi (1550 nm, )', color='blue', linewidth=2)\n",
    "    plt.plot(time_indices, predictions[:, 0], label='Predicted Phi (from 1552 nm, )', color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Phi Time Series (RMSE: {rmse_phi:.2f})')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Phi (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(time_indices, actuals[:, 1], label='Actual Theta (1550 nm, )', color='blue', linewidth=2)\n",
    "    plt.plot(time_indices, predictions[:, 1], label='Predicted Theta (from 1552 nm, )', color='red', linestyle='--', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Theta Time Series (RMSE: {rmse_theta:.2f})')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Theta (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    \n",
    "    errors = np.abs(predictions - actuals)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(time_indices, errors[:, 0], label='|Predicted - Actual| Phi', color='purple', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Phi Absolute Error')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Error (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(time_indices, errors[:, 1], label='|Predicted - Actual| Theta', color='purple', linewidth=2)\n",
    "    plt.title(f'{title_prefix} Theta Absolute Error')\n",
    "    plt.xlabel('Time Index')\n",
    "    plt.ylabel('Error (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'errors_{filename.split(\"_\", 1)[1]}')\n",
    "    plt.close()\n",
    "\n",
    "def experiment_window_sizes(data_path: str, window_sizes: list, batch_size: int = 64, epochs: int = 50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = {}\n",
    "    \n",
    "    for window_size in window_sizes:\n",
    "        print(f\"\\nExperimenting with window size: {window_size}\")\n",
    "        \n",
    "        dataset_full = AngleDataset(data_path, window_size)\n",
    "        total_length = len(dataset_full)\n",
    "        train_length = int(0.8 * total_length)\n",
    "        test_length = total_length - train_length\n",
    "        \n",
    "        train_indices = np.arange(train_length)\n",
    "        test_indices = np.arange(train_length, total_length)\n",
    "        \n",
    "        train_dataset = AngleDataset(data_path, window_size, train_indices)\n",
    "        test_dataset = AngleDataset(data_path, window_size, test_indices)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = AnglePredictionModel(\n",
    "            input_dim=2,\n",
    "            d_model=32,\n",
    "            n_heads=4,\n",
    "            n_layers=2,\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "        \n",
    "        train_losses, test_losses = train_model(\n",
    "            model, train_loader, test_loader, epochs, device\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        time_indices = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_indices in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                output = output.cpu().numpy() * 180.0\n",
    "                batch_y = batch_y.cpu().numpy() * 180.0\n",
    "                predictions.append(output)\n",
    "                actuals.append(batch_y)\n",
    "                time_indices.append(batch_indices.numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        actuals = np.concatenate(actuals, axis=0)\n",
    "        time_indices = np.concatenate(time_indices, axis=0)\n",
    "        \n",
    "        sort_idx = np.argsort(time_indices)\n",
    "        time_indices = time_indices[sort_idx]\n",
    "        predictions = predictions[sort_idx]\n",
    "        actuals = actuals[sort_idx]\n",
    "        \n",
    "        rmse_phi = np.sqrt(np.mean((predictions[:, 0] - actuals[:, 0])**2))\n",
    "        rmse_theta = np.sqrt(np.mean((predictions[:, 1] - actuals[:, 1])**2))\n",
    "        \n",
    "        plot_predictions(\n",
    "            predictions, actuals, time_indices, rmse_phi, rmse_theta,\n",
    "            f'predictions_window_{window_size}.png',\n",
    "            f'Window Size {window_size}'\n",
    "        )\n",
    "        \n",
    "        results[window_size] = {\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals,\n",
    "            'time_indices': time_indices,\n",
    "            'rmse_phi': rmse_phi,\n",
    "            'rmse_theta': rmse_theta\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.title(f'Loss Curves (Window Size: {window_size})')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_window_{window_size}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for window_size, result in results.items():\n",
    "        plt.plot(result['test_losses'], label=f'Window {window_size}')\n",
    "    plt.title('Test Loss Comparison Across Window Sizes')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('window_size_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def experiment_model_sizes(data_path: str, window_size: int = 64, batch_size: int = 64, epochs: int = 50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_configs = [\n",
    "        {'name': 'small', 'd_model': 16, 'n_heads': 4, 'n_layers': 1},\n",
    "        {'name': 'medium', 'd_model': 32, 'n_heads': 4, 'n_layers': 2},\n",
    "        {'name': 'large', 'd_model': 64, 'n_heads': 8, 'n_layers': 3}\n",
    "    ]\n",
    "    results = {}\n",
    "    \n",
    "    dataset_full = AngleDataset(data_path, window_size)\n",
    "    total_length = len(dataset_full)\n",
    "    train_length = int(0.8 * total_length)\n",
    "    test_length = total_length - train_length\n",
    "    \n",
    "    train_indices = np.arange(train_length)\n",
    "    test_indices = np.arange(train_length, total_length)\n",
    "    \n",
    "    train_dataset = AngleDataset(data_path, window_size, train_indices)\n",
    "    test_dataset = AngleDataset(data_path, window_size, test_indices)\n",
    "    \n",
    "    for config in model_configs:\n",
    "        model_name = config['name']\n",
    "        print(f\"\\nExperimenting with model size: {model_name} (d_model={config['d_model']}, \"\n",
    "              f\"n_layers={config['n_layers']}, n_heads={config['n_heads']})\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        model = AnglePredictionModel(\n",
    "            input_dim=2,\n",
    "            d_model=config['d_model'],\n",
    "            n_heads=config['n_heads'],\n",
    "            n_layers=config['n_layers'],\n",
    "            dropout=0.1\n",
    "        ).to(device)\n",
    "        \n",
    "        train_losses, test_losses = train_model(\n",
    "            model, train_loader, test_loader, epochs, device\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        time_indices = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y, batch_indices in test_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    output = model(batch_x)\n",
    "                output = output.cpu().numpy() * 180.0\n",
    "                batch_y = batch_y.cpu().numpy() * 180.0\n",
    "                predictions.append(output)\n",
    "                actuals.append(batch_y)\n",
    "                time_indices.append(batch_indices.numpy())\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        actuals = np.concatenate(actuals, axis=0)\n",
    "        time_indices = np.concatenate(time_indices, axis=0)\n",
    "        \n",
    "        sort_idx = np.argsort(time_indices)\n",
    "        time_indices = time_indices[sort_idx]\n",
    "        predictions = predictions[sort_idx]\n",
    "        actuals = actuals[sort_idx]\n",
    "        \n",
    "        rmse_phi = np.sqrt(np.mean((predictions[:, 0] - actuals[:, 0])**2))\n",
    "        rmse_theta = np.sqrt(np.mean((predictions[:, 1] - actuals[:, 1])**2))\n",
    "        \n",
    "        plot_predictions(\n",
    "            predictions, actuals, time_indices, rmse_phi, rmse_theta,\n",
    "            f'predictions_model_{model_name}.png',\n",
    "            f'Model {model_name.capitalize()} (Window Size 64)'\n",
    "        )\n",
    "        \n",
    "        results[model_name] = {\n",
    "            'train_losses': train_losses,\n",
    "            'test_losses': test_losses,\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'actuals': actuals,\n",
    "            'time_indices': time_indices,\n",
    "            'rmse_phi': rmse_phi,\n",
    "            'rmse_theta': rmse_theta\n",
    "        }\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(test_losses, label='Test Loss')\n",
    "        plt.title(f'Loss Curves (Model: {model_name}, Window Size: 64)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'loss_model_{model_name}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for model_name, result in results.items():\n",
    "        plt.plot(result['test_losses'], label=f'Model {model_name}')\n",
    "    plt.title('Test Loss Comparison Across Model Sizes (Window Size: 64)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('model_size_comparison_window_64.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = os.path.join(\"..\", \"Data\", \"data_angles\", \"data4_angles.csv\")\n",
    "    \n",
    "    print(\"Attempting to access:\", data_path)\n",
    "    print(\"Full path:\", os.path.abspath(data_path))\n",
    "    if os.path.exists(data_path):\n",
    "        print(\"File found!\")\n",
    "    else:\n",
    "        print(\"File not found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Experiment 1: Window sizes\n",
    "    window_sizes = [8, 16, 32, 64, 128]\n",
    "    window_results = experiment_window_sizes(data_path, window_sizes)\n",
    "    \n",
    "    # Experiment 2: Model sizes for window_size=64\n",
    "    model_results = experiment_model_sizes(data_path, window_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19e251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c735fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59a43c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0823cc17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
